#!/usr/bin/env python3
"""
Project Cleanup and Optimization Script
–ê—É–¥–∏—Ç, —á–∏—Å—Ç–∫–∞ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞
"""

import os
import shutil
import json
import logging
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Set, Tuple

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class ProjectCleanup:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞—É–¥–∏—Ç–∞ –∏ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.removed_files = []
        self.removed_dirs = []
        self.archived_files = []
        self.optimized_files = []
        self.duplicates_found = []
        self.unused_files = []
        
    def scan_project_structure(self) -> Dict:
        """–°–∫–∞–Ω–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞"""
        logger.info("üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞...")
        
        structure = {
            'files': [],
            'dirs': [],
            'file_sizes': {},
            'duplicates': [],
            'cache_dirs': [],
            'temp_files': [],
            'large_files': [],
            'empty_files': []
        }
        
        for root, dirs, files in os.walk(self.project_root):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
            if '.venv' in root or '.venv_new' in root:
                continue
                
            rel_root = Path(root).relative_to(self.project_root)
            
            for dir_name in dirs:
                dir_path = Path(root) / dir_name
                structure['dirs'].append(str(rel_root / dir_name))
                
                # –ü–æ–∏—Å–∫ –∫—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
                if dir_name in ['__pycache__', '.pytest_cache', '.mypy_cache', '.coverage']:
                    structure['cache_dirs'].append(str(dir_path))
            
            for file_name in files:
                file_path = Path(root) / file_name
                rel_file_path = rel_root / file_name
                structure['files'].append(str(rel_file_path))
                
                # –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                try:
                    file_size = file_path.stat().st_size
                    structure['file_sizes'][str(rel_file_path)] = file_size
                    
                    if file_size == 0:
                        structure['empty_files'].append(str(rel_file_path))
                    elif file_size > 10 * 1024 * 1024:  # > 10MB
                        structure['large_files'].append(str(rel_file_path))
                        
                except OSError:
                    continue
                
                # –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                if any(ext in file_name.lower() for ext in ['.tmp', '.bak', '.old', '.log', '.out']):
                    structure['temp_files'].append(str(rel_file_path))
        
        return structure
    
    def find_duplicates(self, structure: Dict) -> List[Tuple[str, str]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã —Ñ–∞–π–ª–æ–≤"""
        logger.info("üîç –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤...")
        
        duplicates = []
        file_groups = {}
        
        for file_path, size in structure['file_sizes'].items():
            if size == 0:
                continue
                
            file_name = Path(file_path).name
            if file_name not in file_groups:
                file_groups[file_name] = []
            file_groups[file_name].append(file_path)
        
        for file_name, file_paths in file_groups.items():
            if len(file_paths) > 1:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–æ–≤
                for i, path1 in enumerate(file_paths):
                    for path2 in file_paths[i+1:]:
                        if self._files_are_identical(path1, path2):
                            duplicates.append((path1, path2))
        
        return duplicates
    
    def _files_are_identical(self, path1: str, path2: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–æ–≤"""
        try:
            file1 = self.project_root / path1
            file2 = self.project_root / path2
            
            if file1.stat().st_size != file2.stat().st_size:
                return False
            
            with open(file1, 'rb') as f1, open(file2, 'rb') as f2:
                return f1.read() == f2.read()
        except OSError:
            return False
    
    def find_unused_files(self, structure: Dict) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∞–π–ª—ã"""
        logger.info("üîç –ü–æ–∏—Å–∫ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        unused = []
        main_package = 'tv_generator'
        
        # –§–∞–π–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —Ç–æ—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è
        used_files = {
            'pyproject.toml', 'requirements.txt', 'requirements-dev.txt',
            'README.md', 'LICENSE', '.gitignore', 'pytest.ini',
            'conftest.py', 'Dockerfile', 'CHANGELOG.md', 'AGENTS.md',
            'TODO.md', 'field_type_mapping.json', '.flake8',
            '.pre-commit-config.yaml'
        }
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
        main_scripts = {
            'enhanced_generator.py', 'generate_openapi.py', 'openapi_updater.py',
            'validation_and_ci.py', 'run_enhanced_generator.py'
        }
        
        for file_path in structure['files']:
            file_name = Path(file_path).name
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –≤ –æ—Å–Ω–æ–≤–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
            if any(part in file_path for part in [main_package, 'tests/', 'scripts/', 'docs/']):
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∞–π–ª—ã
            if file_name in used_files or file_name in main_scripts:
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            if 'specs/' in file_path or 'results/' in file_path:
                continue
                
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ª–æ–≥–∏
            if 'logs/' in file_path:
                continue
                
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –¥–µ–º–æ –∏–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–º
            if any(keyword in file_name.lower() for keyword in ['demo', 'test', 'example', 'temp', 'tmp']):
                unused.append(file_path)
        
        return unused
    
    def analyze_documentation(self, structure: Dict) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –ø—Ä–æ–µ–∫—Ç–∞"""
        logger.info("üìö –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏...")
        
        docs = {
            'readme_files': [],
            'markdown_files': [],
            'rst_files': [],
            'empty_docs': [],
            'duplicate_docs': []
        }
        
        for file_path in structure['files']:
            if file_path.endswith('.md'):
                docs['markdown_files'].append(file_path)
                if Path(file_path).name.lower() == 'readme.md':
                    docs['readme_files'].append(file_path)
            elif file_path.endswith('.rst'):
                docs['rst_files'].append(file_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—É—Å—Ç—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        for doc_file in docs['markdown_files'] + docs['rst_files']:
            try:
                file_path = self.project_root / doc_file
                if file_path.stat().st_size < 100:  # < 100 –±–∞–π—Ç
                    docs['empty_docs'].append(doc_file)
            except OSError:
                continue
        
        return docs
    
    def cleanup_cache_directories(self, structure: Dict) -> int:
        """–û—á–∏—â–∞–µ—Ç –∫—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –∫—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π...")
        
        removed_count = 0
        
        for cache_dir in structure['cache_dirs']:
            try:
                cache_path = Path(cache_dir)
                if cache_path.exists():
                    shutil.rmtree(cache_path)
                    self.removed_dirs.append(str(cache_path))
                    removed_count += 1
                    logger.info(f"–£–¥–∞–ª–µ–Ω–∞ –∫—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {cache_path}")
            except OSError as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {cache_dir}: {e}")
        
        return removed_count
    
    def cleanup_temp_files(self, structure: Dict) -> int:
        """–û—á–∏—â–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        removed_count = 0
        
        for temp_file in structure['temp_files']:
            try:
                file_path = self.project_root / temp_file
                if file_path.exists():
                    file_path.unlink()
                    self.removed_files.append(str(file_path))
                    removed_count += 1
                    logger.info(f"–£–¥–∞–ª–µ–Ω –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {file_path}")
            except OSError as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {temp_file}: {e}")
        
        return removed_count
    
    def cleanup_empty_files(self, structure: Dict) -> int:
        """–û—á–∏—â–∞–µ—Ç –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã"""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        removed_count = 0
        
        for empty_file in structure['empty_files']:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∞–∂–Ω—ã–µ –ø—É—Å—Ç—ã–µ —Ñ–∞–π–ª—ã
            if any(skip in empty_file for skip in ['__init__.py', '.gitkeep']):
                continue
                
            try:
                file_path = self.project_root / empty_file
                if file_path.exists():
                    file_path.unlink()
                    self.removed_files.append(str(file_path))
                    removed_count += 1
                    logger.info(f"–£–¥–∞–ª–µ–Ω –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª: {file_path}")
            except OSError as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å {empty_file}: {e}")
        
        return removed_count
    
    def archive_old_files(self, structure: Dict) -> int:
        """–ê—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç —Å—Ç–∞—Ä—ã–µ —Ñ–∞–π–ª—ã"""
        logger.info("üì¶ –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤...")
        
        archived_count = 0
        archive_dir = self.project_root / 'legacy'
        archive_dir.mkdir(exist_ok=True)
        
        # –§–∞–π–ª—ã –¥–ª—è –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏—è
        files_to_archive = [
            'demo_enhanced_generator.py',
            'demo_enhanced_system.py',
            'ENHANCED_GENERATOR_IMPLEMENTATION.md',
            'ENHANCED_IMPLEMENTATION_COMPLETE.md',
            'FINAL_REPORT.md',
            'README_ENHANCED.md',
            'requirements-enhanced.txt'
        ]
        
        for file_name in files_to_archive:
            file_path = self.project_root / file_name
            if file_path.exists():
                try:
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤ –∞—Ä—Ö–∏–≤
                    archive_path = archive_dir / file_name
                    shutil.move(str(file_path), str(archive_path))
                    self.archived_files.append(str(file_path))
                    archived_count += 1
                    logger.info(f"–ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {file_path}")
                except OSError as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞—Ç—å {file_name}: {e}")
        
        return archived_count
    
    def optimize_project_structure(self) -> Dict:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞"""
        logger.info("üîß –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞...")
        
        optimizations = {
            'moved_files': [],
            'created_dirs': [],
            'consolidated_files': []
        }
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —É—Ç–∏–ª–∏—Ç
        utils_dir = self.project_root / 'scripts' / 'utils'
        utils_dir.mkdir(exist_ok=True)
        optimizations['created_dirs'].append(str(utils_dir))
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã
        utility_scripts = [
            'validation_and_ci.py',
            'openapi_updater.py'
        ]
        
        for script in utility_scripts:
            script_path = self.project_root / script
            if script_path.exists():
                try:
                    new_path = utils_dir / script
                    shutil.move(str(script_path), str(new_path))
                    optimizations['moved_files'].append((str(script_path), str(new_path)))
                    logger.info(f"–ü–µ—Ä–µ–º–µ—â–µ–Ω —Å–∫—Ä–∏–ø—Ç: {script_path} -> {new_path}")
                except OSError as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å {script}: {e}")
        
        return optimizations
    
    def generate_cleanup_report(self, structure: Dict, duplicates: List, unused: List, docs: Dict) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ —á–∏—Å—Ç–∫–µ"""
        logger.info("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –æ —á–∏—Å—Ç–∫–µ...")
        
        report = []
        report.append("# PROJECT_CLEANUP_REPORT.md")
        report.append("")
        report.append(f"## –û—Ç—á–µ—Ç –æ –∞—É–¥–∏—Ç–µ –∏ –æ—á–∏—Å—Ç–∫–µ –ø—Ä–æ–µ–∫—Ç–∞")
        report.append(f"**–î–∞—Ç–∞:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"**–ü—Ä–æ–µ–∫—Ç:** {self.project_root.name}")
        report.append("")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report.append("## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞")
        report.append(f"- **–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤:** {len(structure['files'])}")
        report.append(f"- **–í—Å–µ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:** {len(structure['dirs'])}")
        report.append(f"- **–ö—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:** {len(structure['cache_dirs'])}")
        report.append(f"- **–í—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤:** {len(structure['temp_files'])}")
        report.append(f"- **–ü—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤:** {len(structure['empty_files'])}")
        report.append(f"- **–ë–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤ (>10MB):** {len(structure['large_files'])}")
        report.append("")
        
        # –î—É–±–ª–∏–∫–∞—Ç—ã
        if duplicates:
            report.append("## üîÑ –ù–∞–π–¥–µ–Ω–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã")
            for dup1, dup2 in duplicates:
                report.append(f"- `{dup1}` = `{dup2}`")
            report.append("")
        
        # –ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∞–π–ª—ã
        if unused:
            report.append("## üóëÔ∏è –ù–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ñ–∞–π–ª—ã")
            for file_path in unused:
                report.append(f"- `{file_path}`")
            report.append("")
        
        # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
        report.append("## üìö –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏")
        report.append(f"- **README —Ñ–∞–π–ª–æ–≤:** {len(docs['readme_files'])}")
        report.append(f"- **Markdown —Ñ–∞–π–ª–æ–≤:** {len(docs['markdown_files'])}")
        report.append(f"- **RST —Ñ–∞–π–ª–æ–≤:** {len(docs['rst_files'])}")
        report.append(f"- **–ü—É—Å—Ç—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {len(docs['empty_docs'])}")
        report.append("")
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—á–∏—Å—Ç–∫–∏
        report.append("## üßπ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—á–∏—Å—Ç–∫–∏")
        report.append(f"- **–£–¥–∞–ª–µ–Ω–æ —Ñ–∞–π–ª–æ–≤:** {len(self.removed_files)}")
        report.append(f"- **–£–¥–∞–ª–µ–Ω–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:** {len(self.removed_dirs)}")
        report.append(f"- **–ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤:** {len(self.archived_files)}")
        report.append("")
        
        if self.removed_files:
            report.append("### –£–¥–∞–ª–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for file_path in self.removed_files:
                report.append(f"- `{file_path}`")
            report.append("")
        
        if self.removed_dirs:
            report.append("### –£–¥–∞–ª–µ–Ω–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏:")
            for dir_path in self.removed_dirs:
                report.append(f"- `{dir_path}`")
            report.append("")
        
        if self.archived_files:
            report.append("### –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
            for file_path in self.archived_files:
                report.append(f"- `{file_path}`")
            report.append("")
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        report.append("## üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
        report.append("")
        report.append("### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:")
        report.append("- ‚úÖ `src/tv_generator/` - –æ—Å–Ω–æ–≤–Ω–æ–π –ø–∞–∫–µ—Ç")
        report.append("- ‚úÖ `tests/` - —Ç–µ—Å—Ç—ã")
        report.append("- ‚úÖ `scripts/` - –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã")
        report.append("- ‚úÖ `docs/` - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è")
        report.append("- ‚úÖ `specs/` - —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        report.append("- ‚úÖ `results/` - —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
        report.append("- ‚úÖ `logs/` - –ª–æ–≥–∏")
        report.append("")
        
        report.append("### –§–∞–π–ª—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:")
        report.append("- ‚úÖ `pyproject.toml` - –æ—Å–Ω–æ–≤–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è")
        report.append("- ‚úÖ `requirements.txt` - –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        report.append("- ‚úÖ `requirements-dev.txt` - dev –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        report.append("- ‚úÖ `.gitignore` - –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º—ã–µ —Ñ–∞–π–ª—ã")
        report.append("- ‚úÖ `README.md` - –æ—Å–Ω–æ–≤–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è")
        report.append("")
        
        report.append("### –û—Å–Ω–æ–≤–Ω—ã–µ —Å–∫—Ä–∏–ø—Ç—ã:")
        report.append("- ‚úÖ `enhanced_generator.py` - –æ—Å–Ω–æ–≤–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä")
        report.append("- ‚úÖ `generate_openapi.py` - –±–∞–∑–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä")
        report.append("- ‚úÖ `run_enhanced_generator.py` - —Å–∫—Ä–∏–ø—Ç –∑–∞–ø—É—Å–∫–∞")
        report.append("")
        
        return "\n".join(report)
    
    def run_cleanup(self) -> str:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–æ–ª–Ω—É—é –æ—á–∏—Å—Ç–∫—É –ø—Ä–æ–µ–∫—Ç–∞"""
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∞—É–¥–∏—Ç–∞ –∏ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞...")
        
        # –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        structure = self.scan_project_structure()
        
        # –ü–æ–∏—Å–∫ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        duplicates = self.find_duplicates(structure)
        
        # –ü–æ–∏—Å–∫ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö —Ñ–∞–π–ª–æ–≤
        unused = self.find_unused_files(structure)
        
        # –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
        docs = self.analyze_documentation(structure)
        
        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        cache_removed = self.cleanup_cache_directories(structure)
        
        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
        temp_removed = self.cleanup_temp_files(structure)
        
        # –û—á–∏—Å—Ç–∫–∞ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤
        empty_removed = self.cleanup_empty_files(structure)
        
        # –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤
        archived = self.archive_old_files(structure)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        optimizations = self.optimize_project_structure()
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
        report = self.generate_cleanup_report(structure, duplicates, unused, docs)
        
        logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
        logger.info(f"   - –£–¥–∞–ª–µ–Ω–æ –∫—ç—à-–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π: {cache_removed}")
        logger.info(f"   - –£–¥–∞–ª–µ–Ω–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {temp_removed}")
        logger.info(f"   - –£–¥–∞–ª–µ–Ω–æ –ø—É—Å—Ç—ã—Ö —Ñ–∞–π–ª–æ–≤: {empty_removed}")
        logger.info(f"   - –ê—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {archived}")
        
        return report

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    project_root = Path.cwd()
    
    print("üßπ –ê–£–î–ò–¢, –ß–ò–°–¢–ö–ê –ò –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –°–¢–†–£–ö–¢–£–†–´ –ü–†–û–ï–ö–¢–ê")
    print("=" * 60)
    print()
    
    cleanup = ProjectCleanup(project_root)
    report = cleanup.run_cleanup()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
    report_path = project_root / "PROJECT_CLEANUP_REPORT.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print(f"üìÑ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {report_path}")
    print()
    print(report)

if __name__ == "__main__":
    main() 